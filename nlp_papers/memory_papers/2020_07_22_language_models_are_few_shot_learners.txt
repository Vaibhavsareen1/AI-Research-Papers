============================================================================================================================================
Information about the paper
============================================================================================================================================
Paper Name: Language models are few shot learners
By: Aleksandra Piktus, Fabio Petroni, Vladmir Karapukhin et al
Paper Link: https://arxiv.org/pdf/2005.14165
Paper Submission Date: 22nd of July 2020
============================================================================================================================================
Why this paper?
============================================================================================================================================
This paper was used to understand concepts such as zero shot, one shot, few shot prompt tuning to get the desired output. The paper also 
provides insights into how training LLM models on chain of thought datasets help in reasoning capabilities of the model. The training of
model on CoT datasets also improve the capabilities of the llm in terms of remembering over long context.
============================================================================================================================================
Content - 1. Introduction
============================================================================================================================================
In the recent years there has been a trend towards pre trained language representations in NLP systems,  applied in increasingly flexible and
task agnostic ways for downstream transfer. The order in which pre trained language models came about is in the early stage single layer
representation were learned using word vectors which fed to task specific architectures -> RNNs with multiple layers of representations and 
contextual state were used to used to form stronger representations these were still applied to task specific architectures -> we moved to pre
trained recurrent or transformer based language models which have been directly fine tuned, entirely removing the need for task specific 
architectures.

Pre trained language models have made substantial progress on many challenges in NLP however there still lied a major limitation according to
the authors. While the architecture is task agnostic, there is still a need for task specific datasets and task specific fine tuning to achieve
strong performance on a desired task. According to authors removing this limitation would be desirable for several reasons.

1. From a practical perspective the need for a large dataset of labeled examples for every new task limits the applicability of language models.
2. There is potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the 
   narrowness of the training distribution.
   - This can create problem for the pre training plus fine tuning paradigm where models are designed to be large to absorb information during
     pre training but then fine tuned on very narrow task distributions.
   - It has been observed that larger models do not necessarily generalize better out of distribution.
   - There is evidence that suggests that generalization achieved under this paradigm can be poor because the model is overly specific to the 
     training distribution and does not generalize well outside of it, thus the performance of fine tuned models on specific benchmarks, may 
     exaggerate actual performance on the underlying task.
3. Humans do not require large supervised datasets to learn most language tasks or at most a tiny number of demonstrations is often sufficient
   to enable a human to perform a new task to atleast reasonable degree of competence.

Few of the advantages of LLMs is that it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing
additional during lengthy dialogue.

According to authors one potential route towards addressing these issues is meta-learning in the context of language models means the model 
develops a broad set of skills and pattern recognition abilities at training time and then use those abilities at inference time to rapidly 
adapt to or recognize the desired task. Meta learning in terms of language model has been researched as 'in context learning', using the text
input of a pretrained language model as a form of task specification. The model is conditioned on a natural language instruction and/or a few
demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next. This approach
shows some initial promise but still achieves results inferior to fine tuning.

Authors test the 'in context learning' paradigm in this paper using GPT-3 with 175 Billion parameters. They evaluate the model on 2 dozen NLP
dataset and on certain novel NLP tasks. For each task the model GPT-3 is evaluated on 3 conditions
1. Few shot learning
2. One shot learning
3. Zero shot learning

The authors through their testing find that few shot learning performs better than one shot and zero shot learning.
============================================================================================================================================
Content - 2. Approach
============================================================================================================================================
The authors use different settings in order to test their large language model on in the 'in context learning' paradigm. These settings are
to see how much task specific data the model relies on to produce results.

2.0.1 Fine Tuning:
Fine tuning has been the most common approach which involves updating the weights of a pretrained model by training on a supervised dataset
specific to the desired task. The main advantage of fine tuning is strong performance on many benchmarks. The main disadvantage are the need 
for a new large dataset for every task, the potential for poor generalization out of distribution and the potential to exploit spurious
features of the training data, potentially resulting in an unfair comparison with human performance.

2.0.2 Few-Shot:
The term refers to the setting where the model is given a few demonstrations of the task at inference time as conditioning but no weight
updates are allowed. For a typical dataset an example has a context and a desired completion and few shot works giving K examples of context 
and completion and then one final example of context with the model expected to provide the completion. K for them is set between 10 to 100.
Main disadvantage is that few task specific data is still required as indicated by the name few shot.

2.0.3 One-Shot:
This is the same as few shot except only one demonstration is allowed, in addition to a natural language description of the task.

2.0.4 Zero-Shot:
This is the same as one shot except that no demonstrations are allowed and then the model is only given a natural language instruction describing
the task. This method provides maximum convenience, potential for robustness and avoidance of spurious correlations, but is also the most
challenging setting.

2.1 Model and Architectures
Not needed

2.2 Training Dataset:
Not needed

2.3 Training Process
Not needed

2.4 Evaluation
Not needed
============================================================================================================================================
Content - 3. Results and so on
============================================================================================================================================
Not needed